#!/bin/bash
# Hot-swap between Llama 3.1 8B and GLM-4.7-Flash
# Usage: ./scripts/swap-model.sh [llama|glm]

set -e

MODEL="${1:-llama}"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

function log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

function log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

function log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

function log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Determine model configuration
case "$MODEL" in
    llama|l)
        FAST_MODEL="llama3.1:8b"
        QUALITY_MODEL="llama3.3"
        MODEL_NAME="Llama 3.1 8B"
        ;;
    glm|g)
        FAST_MODEL="glm-4.7-flash"
        QUALITY_MODEL="glm-4.7-flash"
        MODEL_NAME="GLM-4.7-Flash"
        ;;
    *)
        log_error "Invalid model: $MODEL"
        echo "Usage: $0 [llama|glm]"
        exit 1
        ;;
esac

echo ""
echo "=========================================="
echo "  Model Hot-Swap: $MODEL_NAME"
echo "=========================================="
echo ""

log_info "Fast model: $FAST_MODEL"
log_info "Quality model: $QUALITY_MODEL"
echo ""

# Step 1: Export environment variables for current shell
log_info "Step 1: Setting environment variables..."
export OLLAMA_MODEL_FAST="$FAST_MODEL"
export OLLAMA_MODEL_QUALITY="$QUALITY_MODEL"
log_success "Environment variables set"

# Step 2: Update .env file if it exists (for persistence)
ENV_FILE="$(dirname "$0")/../.env"
if [ -f "$ENV_FILE" ]; then
    log_info "Step 2: Updating .env file..."

    # Backup .env
    cp "$ENV_FILE" "$ENV_FILE.backup.$(date +%Y%m%d_%H%M%S)"

    # Update or append OLLAMA_MODEL_FAST
    if grep -q "^OLLAMA_MODEL_FAST=" "$ENV_FILE"; then
        sed -i "s|^OLLAMA_MODEL_FAST=.*|OLLAMA_MODEL_FAST=$FAST_MODEL|" "$ENV_FILE"
    else
        echo "OLLAMA_MODEL_FAST=$FAST_MODEL" >> "$ENV_FILE"
    fi

    # Update or append OLLAMA_MODEL_QUALITY
    if grep -q "^OLLAMA_MODEL_QUALITY=" "$ENV_FILE"; then
        sed -i "s|^OLLAMA_MODEL_QUALITY=.*|OLLAMA_MODEL_QUALITY=$QUALITY_MODEL|" "$ENV_FILE"
    else
        echo "OLLAMA_MODEL_QUALITY=$QUALITY_MODEL" >> "$ENV_FILE"
    fi

    log_success ".env file updated"
else
    log_warn ".env file not found, creating it..."
    cat > "$ENV_FILE" << EOF
# Ollama model configuration (auto-generated by swap-model.sh)
OLLAMA_MODEL_FAST=$FAST_MODEL
OLLAMA_MODEL_QUALITY=$QUALITY_MODEL
EOF
    log_success ".env file created"
fi

# Step 3: Find and restart backend processes
log_info "Step 3: Restarting backend services..."

# Find gunicorn processes
GUNICORN_PIDS=$(pgrep -f "gunicorn.*8100" || true)

if [ ! -z "$GUNICORN_PIDS" ]; then
    log_info "Found gunicorn processes: $GUNICORN_PIDS"

    # Kill gunicorn gracefully
    for pid in $GUNICORN_PIDS; do
        log_info "Stopping gunicorn (PID: $pid)..."
        kill "$pid" 2>/dev/null || true
    done

    # Wait for processes to stop
    sleep 2

    # Start gunicorn with new env vars
    log_info "Starting gunicorn with $MODEL_NAME..."
    cd "$(dirname "$0")/../backend"

    ENABLE_CONTINUOUS_RL=true \
    GUNICORN_WORKER=true \
    OLLAMA_MODEL_FAST="$FAST_MODEL" \
    OLLAMA_MODEL_QUALITY="$QUALITY_MODEL" \
    gunicorn command_center.wsgi:application \
        --bind 127.0.0.1:8100 \
        --workers 4 \
        --timeout 120 \
        --daemon \
        --access-logfile /tmp/gunicorn-access.log \
        --error-logfile /tmp/gunicorn-error.log

    sleep 2
    log_success "Backend restarted"
else
    log_warn "No gunicorn process found on port 8100"
    log_info "If using dev server, restart manually with:"
    echo "    cd backend && OLLAMA_MODEL_FAST=$FAST_MODEL OLLAMA_MODEL_QUALITY=$QUALITY_MODEL python manage.py runserver 8100"
fi

# Step 4: Verify the swap
log_info "Step 4: Verifying model swap..."
sleep 1

# Check if backend is responding
if curl -s http://127.0.0.1:8100/api/layer2/health/ > /dev/null 2>&1; then
    log_success "Backend is responding"

    # Try to get current model info
    RESPONSE=$(curl -s http://127.0.0.1:8100/api/layer2/orchestrate/ \
        -H "Content-Type: application/json" \
        -d '{"transcript":"test"}' 2>/dev/null || echo "")

    if [ ! -z "$RESPONSE" ]; then
        log_success "Orchestrator is working"
    else
        log_warn "Could not verify orchestrator (this is normal if query failed)"
    fi
else
    log_error "Backend is not responding on http://127.0.0.1:8100"
    log_info "Check logs: tail -f /tmp/gunicorn-error.log"
fi

# Step 5: Test Ollama directly
log_info "Step 5: Testing Ollama model..."
OLLAMA_TEST=$(curl -s http://localhost:11434/api/generate -d "{
  \"model\": \"$FAST_MODEL\",
  \"prompt\": \"Return JSON: {\\\"status\\\": \\\"ok\\\"}\",
  \"stream\": false,
  \"options\": {\"temperature\": 0, \"num_predict\": 50}
}" | python3 -c "import sys, json; r=json.load(sys.stdin); print('✓' if r.get('response') else '✗')" 2>/dev/null || echo "✗")

if [ "$OLLAMA_TEST" = "✓" ]; then
    log_success "Ollama model $FAST_MODEL is responding"
else
    log_error "Ollama model $FAST_MODEL is not responding"
    log_info "Check if model is pulled: ollama list | grep ${FAST_MODEL%%:*}"
fi

echo ""
echo "=========================================="
echo "  Model Swap Complete!"
echo "=========================================="
echo ""
echo "Current configuration:"
echo "  Fast model:    $FAST_MODEL"
echo "  Quality model: $QUALITY_MODEL"
echo ""
echo "To swap back, run:"
if [ "$MODEL" = "llama" ]; then
    echo "  ./scripts/swap-model.sh glm"
else
    echo "  ./scripts/swap-model.sh llama"
fi
echo ""
