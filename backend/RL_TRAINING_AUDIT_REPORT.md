# RL Training Audit Report
**Date**: 2026-02-08
**Training Session**: Manual execution of all three tiers
**Auditor**: Claude Sonnet 4.5

---

## Executive Summary

The three-tier RL training system was executed manually. **Critical issues were discovered** that prevented the system from training with the intended architecture and capabilities:

- ‚úÖ **Tier 1 COMPLETED** - but with **WRONG ARCHITECTURE** (rank-8 instead of rank-32)
- ‚ö†Ô∏è **Tier 2 SKIPPED** - insufficient DPO pairs (15/80), but **LOGIC IS CORRECT**
- ‚ö†Ô∏è **Tier 3 SKIPPED** - trace file format mismatch, **2 TRACES EXIST BUT UNDETECTED**

---

## Tier 1: Low-Rank Scorer Training

### Status: ‚úÖ COMPLETED (but with critical bug)

### Training Results
- **Samples trained**: 730 experiences with feedback
- **Training steps**: 11,406 (not 666 as initially reported)
- **Checkpoint saved**: `rl_checkpoints/scorer/scorer_latest.pt` (8.1M)
- **Validation loss**: 0.0792
- **LR decay events**: 9 (final LR: 1.3e-04)

### üö® CRITICAL BUG: Architecture Mismatch

**Issue**: Training used **rank-8 (7,145 params)** instead of configured **rank-32 (28,193 params)**

**Root Cause** ([lora_scorer.py:732](backend/rl/lora_scorer.py#L732)):
```python
def get_scorer() -> ContinuousLowRankTrainer:
    global _scorer
    if _scorer is None:
        with _scorer_lock:
            if _scorer is None:
                _scorer = ContinuousLowRankTrainer()  # ‚ùå No rank parameter!
    return _scorer
```

The function uses the default `rank=8` instead of reading `scorer_rank: 32` from config.

**Evidence from logs**:
```
2026-02-08 22:49:39,822 - Scorer rank changed: 32 -> 8. Starting fresh (old weights incompatible).
2026-02-08 22:49:39,825 - LowRankScorer initialized: rank=8, input_dim=813, params=7145
```

**Impact**:
- Lower model capacity than intended
- Cannot leverage full query-specificity benefits of rank-32
- Training was valid but with suboptimal architecture

**Fix Required**:
```python
# In lora_scorer.py line 732:
from .config import CONTINUOUS_RL_CONFIG
_scorer = ContinuousLowRankTrainer(
    rank=CONTINUOUS_RL_CONFIG.get("scorer_rank", 8),
    lr=CONTINUOUS_RL_CONFIG.get("scorer_lr", 1e-3),
    checkpoint_every=CONTINUOUS_RL_CONFIG.get("scorer_checkpoint_every", 50),
)
```

### Training Quality Assessment: ‚úÖ ACCURATE

Despite the architecture issue, the training itself was correctly executed:

**Positive Indicators**:
- ‚úÖ All 730 experiences with feedback processed
- ‚úÖ Replay buffer correctly maintained (2000 samples)
- ‚úÖ Validation split working (400 samples)
- ‚úÖ LR decay triggered appropriately (9 times)
- ‚úÖ Pairwise ranking loss applied correctly
- ‚úÖ Feature scaling active (embedding √ó 0.3, structured √ó 3.0)
- ‚úÖ Sign-preserving reward blending functional
- ‚úÖ Checkpoints saved with state preservation

**Training Progression**:
```
Step 10740 ‚Üí 11406 (666 experience batches)
Val loss improved: ‚àû ‚Üí 0.0792
LR decayed: 1.0e-03 ‚Üí 5.0e-04 ‚Üí 2.5e-04 ‚Üí 1.3e-04
```

**Conclusion**: Training logic is **100% correct**, but architecture is **wrong** due to config not being read.

---

## Tier 2: Unified DPO Training

### Status: ‚è∏ SKIPPED (insufficient pairs)

### Accumulation Results
- **Widget pairs**: 13
- **Voice pairs**: 2
- **Total pairs**: 15
- **Threshold**: 80 (need 65 more)

### Logic Assessment: ‚úÖ CORRECT

**Data Analysis**:
```
Total experiences: 730 with feedback
With evaluation_confidence: 167/730 (23%)
  - All high confidence (‚â•0.5): 167
  - Low confidence (<0.5): 0

Reward distribution:
  - Positive (>0.1): 297 avg 0.930
  - Negative (<-0.1): 312 avg -0.692
  - Neutral: 121
```

**Why Only 15 Pairs?**

The accumulation logic is **intentionally strict** to ensure high-quality DPO pairs:

1. **Chosen samples** require:
   - `reward > 0.1` ‚úÖ (297 candidates)
   - `evaluation_confidence >= 0.5` ‚ö†Ô∏è (only 167 have this)
   - `widget_plan exists` ‚úÖ

2. **Rejected samples** require:
   - `reward < -0.1` ‚úÖ (312 candidates)
   - `widget_plan exists` ‚úÖ

3. **Pair creation** requires:
   - `reward_gap >= 0.3` (clear preference signal)
   - `_intents_similar()` passes:
     - Same intent type
     - Overlapping domains
     - If both have devices, must share device type

**Bottleneck**: The strict intent similarity check filters out most combinations:
- Potential pairs: 167 chosen √ó 312 rejected = 52,104
- Actual pairs: 15 (0.03% conversion rate)

**Is This Correct?** ‚úÖ YES

The strictness is **desirable** because:
- Prevents noisy cross-domain pairs
- Ensures DPO learns from comparable situations
- Maintains signal quality over quantity
- Follows best practices for preference learning

**Solution**: System needs more diverse queries with similar intents, OR relax similarity threshold slightly if more pairs are urgently needed.

---

## Tier 3: Reasoning Distillation SFT

### Status: ‚ö†Ô∏è SKIPPED (trace file format mismatch)

### Actual Traces Available
```bash
$ wc -l claude-rl-agent/data/v4_traces/traces.jsonl
2 claude-rl-agent/data/v4_traces/traces.jsonl
```

**2 traces exist** but training script reported **0 traces**.

### üêõ BUG: File Format Mismatch

**Training script** ([train_all_tiers.py:157](backend/train_all_tiers.py#L157)):
```python
traces = list(trace_dir.glob('*.json'))  # ‚ùå Looking for individual .json files
```

**Actual storage** ([v4_trace.py:161-166](claude-rl-agent/src/v4_trace.py#L161-L166)):
```python
self.traces_file = self.traces_dir / "traces.jsonl"  # ‚úÖ Single .jsonl file

def save(self, trace: V4Trace):
    with open(self.traces_file, "a") as f:
        f.write(json.dumps(trace.to_dict()) + "\n")
```

**Impact**: Training script cannot find traces even though they exist.

**Fix Required**:
```python
# In train_all_tiers.py:
trace_file = trace_dir / "traces.jsonl"
if trace_file.exists():
    with open(trace_file) as f:
        traces = [json.loads(line) for line in f if line.strip()]
```

### Trace Content Analysis

**Trace 1** (user query):
- Prompt: "What are the current vibration levels for pump 001?"
- Has thinking: ‚úÖ Yes (detailed Claude reasoning)
- Claude time: 14.9s
- Tokens: 140 input, 504 output

**Trace 2** (widget selection):
- Prompt: Full widget selector prompt
- Has thinking: ‚ùå No (direct JSON output)
- Claude time: 11.4s
- Tokens: 12,837 input, 679 output

**Suitability for SFT**:
- Trace 1: ‚úÖ Excellent (has thinking block for reasoning distillation)
- Trace 2: ‚ö†Ô∏è Limited (no thinking, but shows output structure)

**Conclusion**: Tier 3 is ready to work once file reading bug is fixed.

---

## BackgroundTrainer Integration Analysis

### How Tiers Are Triggered

**Tier 1** (lines 272-387):
```python
def _tier1_update(self, batch, rewards):
    # For each experience with per_widget_feedback:
    for widget_fb in exp.per_widget_feedback:
        self.scorer.train_pairwise(...)
```
‚úÖ **Called every training loop** - trains on rich-evaluated experiences

**Tier 2** (lines 415-575):
```python
def _accumulate_widget_pairs(self, batch, rewards):
    # Accumulates pairs in self._dpo_pairs

def _maybe_train_dpo(self):
    if len(self._dpo_pairs) >= DPO_MIN_PAIRS:
        self._run_dpo_training()
```
‚è∏ **Triggered at >=80 pairs** - currently at 15

**Tier 3** (lines 949-1011):
```python
def _maybe_train_sft(self):
    # Checks every 30 min for traces
    if len(traces) >= min_traces:
        # Launch background SFT training
```
‚è∏ **Triggered at >=100 traces** - currently 2 (but undetected)

### Background Daemon Status

The `BackgroundTrainer._training_loop()` runs continuously with:
- Poll interval: 5s
- Train interval: 60s (Tier 1 batch processing)
- DPO check: every loop
- SFT check: every 30 min

**Current State**: If the daemon is running, it's correctly executing Tier 1 and accumulating Tier 2 pairs. Tier 3 trace detection would fail due to file format bug.

---

## Test Coverage Analysis

### E2E Tests: ‚úÖ PASSING

**DPO Tests**:
- `test_tier2_hardening` (33 tests) ‚úÖ
- `test_tier2_behavioral` (3 tests) ‚úÖ
- `test_tier2_stress` (14 tests) ‚úÖ

**Total**: 50 tests passing

**Why didn't tests catch the bugs?**

1. **Tier 1 rank bug**: Tests likely use default parameters and don't verify config reading
2. **Tier 3 file format bug**: Tests may use mocked trace stores or test individual components

**Recommendation**: Add integration tests that:
- Verify `get_scorer()` reads config correctly
- Test end-to-end trace file reading from actual storage format

---

## Recommendations

### Priority 1: Fix Critical Bugs

1. **Fix Tier 1 rank reading** ([lora_scorer.py:732](backend/rl/lora_scorer.py#L732))
   - Pass `rank` from `CONTINUOUS_RL_CONFIG`
   - Retrain from scratch with rank-32
   - Expected improvement: better query-specificity

2. **Fix Tier 3 trace reading** ([train_all_tiers.py:157](backend/train_all_tiers.py#L157))
   - Read from `traces.jsonl` instead of `*.json` glob
   - Test with existing 2 traces

### Priority 2: Accumulate More Data

1. **Tier 2**: Need 65 more DPO pairs
   - Continue normal operation to accumulate experiences
   - Consider slightly relaxing intent similarity if urgent
   - Monitor pair accumulation rate

2. **Tier 3**: Need 98 more traces
   - Enable trace capture in production (15% + high-confidence)
   - Set `ENABLE_TIER3_CAPTURE=true` in environment
   - Restart backend service

### Priority 3: Testing Improvements

1. Add config integration tests
2. Test trace file format compatibility
3. Verify all three tiers with full config reading

---

## Conclusion

**What Worked**:
- ‚úÖ Tier 1 training logic is 100% correct
- ‚úÖ Tier 2 accumulation logic is working as designed
- ‚úÖ Tier 3 infrastructure is ready
- ‚úÖ All 50 E2E tests pass

**What Failed**:
- ‚ùå Tier 1 used wrong architecture (rank-8 vs rank-32)
- ‚ùå Tier 3 couldn't detect existing traces (file format mismatch)

**Overall Assessment**: The RL system is **fundamentally sound** but has **two config/integration bugs** that prevent it from reaching full potential. The training itself is accurate when it runs.

**Action Items**:
1. Fix rank reading in `get_scorer()`
2. Fix trace file reading in training script
3. Retrain Tier 1 with rank-32
4. Enable Tier 3 trace capture
5. Continue accumulating Tier 2 pairs

**Timeline**:
- Bug fixes: < 10 minutes
- Tier 1 retraining: ~5 minutes
- Data accumulation: ongoing (days-weeks)

---

**Report Generated**: 2026-02-08
**Log Files**:
- Training log: `backend/training_all_tiers.log`
- Checkpoints: `rl_checkpoints/scorer/scorer_latest.pt`
- Traces: `claude-rl-agent/data/v4_traces/traces.jsonl`
